# Complete Workflow Summary - Silksong Gesture Training

## ğŸ¯ Overview

This document provides a complete overview of the gesture recognition training pipeline with two-stage classification architecture.

## ğŸ“‹ Quick Reference

| Stage | Task | Command | Time | Output |
|-------|------|---------|------|--------|
| 1 | Collect Data | `python src/phase_vi_button_collection/data_collection_dashboard.py` | 10-30 min | CSV files in `data/button_collected/` |
| 2 | Organize Data | `python src/organize_training_data.py` | 1 min | Organized data in `data/organized_training/` |
| 3a | Train SVM (Local) | `python notebooks/SVM_Local_Training.py` | 5-15 min | Models in `models/*.pkl` |
| 3b | Train CNN-LSTM (Colab) | Upload to Colab, run notebook | 20-40 min | Models in Google Drive |
| 4 | Augment Data (Optional) | `python src/data_augmentation.py` | 2 min | Augmented data in `data/organized_training_augmented/` |
| 5 | Integrate Controller | Edit `src/phase_iv_ml_controller/udp_listener.py` | 30 min | Two-stage prediction |
| 6 | Test Controller | `python src/phase_iv_ml_controller/udp_listener.py` | Real-time | Live gesture control |

## ğŸ—ï¸ Architecture

### Two-Stage Classification

```
Sensor Data (50Hz)
    â†“
Feature Extraction (108 features)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: Binary Classifier â”‚
â”‚  (Walking vs Not-Walking)   â”‚
â”‚  Accuracy: 94.29%           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                      â†“
 Walking            Not Walking
    â†“                      â†“
Execute Walk        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ STAGE 2: Multi-class       â”‚
                    â”‚ (Jump, Punch, Turn, Idle)  â”‚
                    â”‚ Accuracy: 57.14% (âš ï¸)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“       â†“       â†“       â†“
                      Jump   Punch   Turn    Idle
                         â†“       â†“       â†“       â†“
                    Execute corresponding action
```

### Why Two-Stage?

1. **Different Gesture Types**: Walking is continuous, actions are discrete
2. **Better Accuracy**: Specialized classifiers for each type
3. **Logical Game Control**: Can't jump/punch/turn while walking
4. **Avoids Data Dominance**: Walking data doesn't overwhelm action training

## ğŸ“ Project Structure

```
v3-watch_SilksongController_25TPE/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ button_collected/              # Raw collected data
â”‚   â”‚   â”œâ”€â”€ jump_*.csv                 # 40 jump samples
â”‚   â”‚   â”œâ”€â”€ punch_*.csv                # 34 punch samples
â”‚   â”‚   â”œâ”€â”€ turn_*.csv                 # 73 turn samples
â”‚   â”‚   â”œâ”€â”€ walk_*.csv                 # 34 walk samples
â”‚   â”‚   â””â”€â”€ idle_*.csv                 # 36 idle samples
â”‚   â””â”€â”€ organized_training/            # Organized for training
â”‚       â”œâ”€â”€ binary_classification/
â”‚       â”‚   â”œâ”€â”€ walking/ (34)
â”‚       â”‚   â””â”€â”€ not_walking/ (139)
â”‚       â”œâ”€â”€ multiclass_classification/
â”‚       â”‚   â”œâ”€â”€ jump/ (35)
â”‚       â”‚   â”œâ”€â”€ punch/ (34)
â”‚       â”‚   â”œâ”€â”€ turn/ (35)
â”‚       â”‚   â””â”€â”€ idle/ (35)
â”‚       â”œâ”€â”€ noise_detection/
â”‚       â”‚   â”œâ”€â”€ idle/ (35)
â”‚       â”‚   â””â”€â”€ active/ (138)
â”‚       â””â”€â”€ metadata.json
â”œâ”€â”€ models/                            # Trained models
â”‚   â”œâ”€â”€ gesture_classifier_binary.pkl
â”‚   â”œâ”€â”€ feature_scaler_binary.pkl
â”‚   â”œâ”€â”€ feature_names_binary.pkl
â”‚   â”œâ”€â”€ gesture_classifier_multiclass.pkl
â”‚   â”œâ”€â”€ feature_scaler_multiclass.pkl
â”‚   â””â”€â”€ feature_names_multiclass.pkl
â”œâ”€â”€ notebooks/                         # Training notebooks
â”‚   â”œâ”€â”€ SVM_Local_Training.ipynb      # Local SVM training
â”‚   â””â”€â”€ Silksong_Complete_Training_Colab.ipynb  # Colab CNN-LSTM
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ organize_training_data.py     # Data organization
â”‚   â”œâ”€â”€ data_augmentation.py          # Data augmentation
â”‚   â”œâ”€â”€ feature_extractor.py          # Feature engineering
â”‚   â””â”€â”€ phase_iv_ml_controller/
â”‚       â””â”€â”€ udp_listener.py           # Real-time controller
â””â”€â”€ docs/
    â”œâ”€â”€ DATA_ORGANIZATION_GUIDE.md    # Complete workflow
    â”œâ”€â”€ TRAINING_RESULTS_SUMMARY.md   # Model performance
    â”œâ”€â”€ TWO_STAGE_CONTROLLER_INTEGRATION.md  # Integration guide
    â””â”€â”€ COMPLETE_WORKFLOW_SUMMARY.md  # This document
```

## ğŸ”„ Complete Workflow

### Phase 1: Data Collection

**Goal**: Collect 30-50 samples per gesture

**Tools**:
- `src/phase_vi_button_collection/data_collection_dashboard.py` - Real-time monitoring
- `src/phase_vi_button_collection/button_data_collector.py` - Simple collection

**Process**:
1. Start Android app on phone (NSD discovery)
2. Start watch app (sensor streaming)
3. Run dashboard: `python data_collection_dashboard.py`
4. Click button for gesture, perform gesture, click stop
5. Repeat for all gestures: jump, punch, turn, walk, idle

**Output**: CSV files in `data/button_collected/`

**Verify**:
```bash
python src/shared_utils/inspect_csv_data.py data/button_collected/*.csv
```

### Phase 2: Data Organization

**Goal**: Structure data for two-stage training with class balancing

**Tool**: `src/organize_training_data.py`

**Process**:
```bash
python src/organize_training_data.py \
  --input data/button_collected \
  --output data/organized_training \
  --target-samples 35
```

**Features**:
- âœ… Analyzes class distribution
- âœ… Undersamples majority classes (turn: 73 â†’ 35)
- âœ… Creates three training sets (binary, multiclass, noise)
- âœ… Generates metadata.json
- âœ… Validates CSV format

**Output**: Organized data in `data/organized_training/`

### Phase 3a: Local Training (SVM)

**Goal**: Train fast SVM models on your machine (no GPU needed)

**Tool**: `notebooks/SVM_Local_Training.ipynb` or `.py`

**Process**:
```bash
# As Jupyter notebook
jupyter notebook notebooks/SVM_Local_Training.ipynb

# Or as Python script
python notebooks/SVM_Local_Training.py
```

**Features**:
- âœ… Trains both binary and multiclass models
- âœ… Feature extraction (108 features: time + frequency domain)
- âœ… Confusion matrices
- âœ… Classification reports
- âœ… Auto-saves models to `models/`

**Training Time**: 5-15 minutes

**Output**: 
- `models/gesture_classifier_binary.pkl` (94% accuracy)
- `models/gesture_classifier_multiclass.pkl` (57% accuracy)

### Phase 3b: Deep Learning (CNN-LSTM on Colab)

**Goal**: Higher accuracy with temporal awareness

**Tool**: `notebooks/Silksong_Complete_Training_Colab.ipynb`

**Process**:
1. Upload `data/organized_training/` to Google Drive at `My Drive/silksong_data/`
2. Open notebook in Colab
3. Enable GPU: Runtime â†’ Change runtime type â†’ GPU
4. Set `TRAINING_MODE = 'MULTICLASS'` (or 'BINARY')
5. Run all cells

**Features**:
- âœ… CNN for spatial feature learning
- âœ… LSTM for temporal modeling
- âœ… Data augmentation built-in
- âœ… Training curves visualization
- âœ… Higher accuracy (85-92% expected)

**Training Time**: 20-40 minutes with T4 GPU

**Expected Improvement**: 57% â†’ 85-92%

### Phase 4: Data Augmentation (Optional)

**Goal**: Expand minority classes to improve accuracy

**Tool**: `src/data_augmentation.py`

**Process**:
```bash
python src/data_augmentation.py \
  --input data/organized_training/multiclass_classification \
  --output data/organized_training_augmented \
  --target-samples 50
```

**Techniques**:
- âœ… Gaussian noise injection
- âœ… Time warping (stretch/compress)
- âœ… Magnitude scaling
- âœ… Time shifting

**Output**: Augmented data in `data/organized_training_augmented/`

### Phase 5: Controller Integration

**Goal**: Use two-stage classifier in real-time controller

**Tool**: `src/phase_iv_ml_controller/udp_listener.py`

**Implementation**: See `docs/TWO_STAGE_CONTROLLER_INTEGRATION.md`

**Key Changes**:
1. Load both binary and multiclass models
2. Implement `predict_two_stage()` function
3. Use separate confidence thresholds
4. Add prediction history for confidence gating

**Code Snippet**:
```python
# Stage 1: Binary
binary_pred = binary_clf.predict(binary_features)[0]
if binary_pred == 0:  # Walking
    return 'WALK'
else:  # Not walking - Stage 2
    multi_pred = multi_clf.predict(multi_features)[0]
    return ['JUMP', 'PUNCH', 'TURN', 'IDLE'][multi_pred]
```

### Phase 6: Testing & Iteration

**Goal**: Validate performance in real gameplay

**Process**:
1. Start watch + phone apps
2. Run controller: `python src/phase_iv_ml_controller/udp_listener.py`
3. Test each gesture type
4. Monitor logs for accuracy
5. Collect more data for low-performing gestures
6. Re-train models
7. Repeat

**Metrics to Track**:
- Walk detection accuracy
- Action recognition accuracy  
- False positive rate
- Latency (<500ms target)

## ğŸ“Š Current Performance

### Binary Classifier
- âœ… **Test Accuracy**: 94.29%
- âœ… **Walking Precision**: 100%
- âœ… **Not-Walking Recall**: 100%
- âœ… **Production Ready**: Yes

### Multi-class Classifier
- âš ï¸ **Test Accuracy**: 57.14%
- âœ… **Turn F1-Score**: 0.80 (best)
- âš ï¸ **Jump F1-Score**: 0.20 (needs work)
- âš ï¸ **Production Ready**: Needs improvement

### Recommendations

**Immediate** (Today):
1. Collect 15-20 more jump samples
2. Collect 15-20 more punch samples
3. Apply data augmentation
4. Re-train models

**Short-term** (This Week):
1. Try CNN-LSTM on Colab
2. Implement two-stage controller
3. Test with live gameplay
4. Iterate based on results

**Long-term** (Next Sprint):
1. Collect 100+ samples per gesture
2. Optimize confidence thresholds
3. Add noise detection
4. Deploy production system

## ğŸ› ï¸ Tools & Scripts

| Script | Purpose | Usage |
|--------|---------|-------|
| `organize_training_data.py` | Organize & balance data | `python src/organize_training_data.py --input data/button_collected` |
| `data_augmentation.py` | Augment minority classes | `python src/data_augmentation.py --input data/organized_training/multiclass_classification` |
| `SVM_Local_Training.py` | Train SVM models | `python notebooks/SVM_Local_Training.py` |
| `inspect_csv_data.py` | Verify CSV format | `python src/shared_utils/inspect_csv_data.py data/button_collected/*.csv` |
| `test_two_stage_classifier.py` | Test two-stage prediction | `python src/test_two_stage_classifier.py` |

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| `DATA_ORGANIZATION_GUIDE.md` | Complete workflow from collection to training |
| `TRAINING_RESULTS_SUMMARY.md` | Model performance analysis |
| `TWO_STAGE_CONTROLLER_INTEGRATION.md` | Controller integration guide |
| `COMPLETE_WORKFLOW_SUMMARY.md` | This document - overview |
| `DASHBOARD_QUICK_START.md` | Data collection dashboard guide |
| `CLASS_BALANCING_GUIDE.md` | Class balancing strategies |

## ğŸ“ Key Concepts

### Feature Engineering
- **Time Domain**: mean, std, min, max, range, median, skew, kurtosis
- **Frequency Domain**: FFT max, dominant frequency
- **Magnitude Features**: accel magnitude, gyro magnitude
- **Total**: 108 features per sample

### Class Balancing
- **Undersampling**: Reduce majority class (turn: 73 â†’ 35)
- **Augmentation**: Expand minority class (add synthetic samples)
- **Class Weights**: Penalize misclassifications of minority class

### Confidence Gating
- **Deque History**: Require N consecutive matching predictions
- **Thresholds**: Minimum confidence per stage (0.6 binary, 0.7 multiclass)
- **Purpose**: Reduce false positives, stabilize control

### Two-Stage Logic
```
Is the user walking?
â”œâ”€ Yes â†’ Execute WALK
â””â”€ No â†’ What action are they doing?
    â”œâ”€ Jump â†’ Execute JUMP
    â”œâ”€ Punch â†’ Execute PUNCH
    â”œâ”€ Turn â†’ Execute TURN
    â””â”€ Idle â†’ Execute IDLE (or no action)
```

## ğŸš€ Getting Started Checklist

- [ ] Data collected (30-50 samples per gesture)
- [ ] Data organized with `organize_training_data.py`
- [ ] Models trained (SVM or CNN-LSTM)
- [ ] Models tested with sample data
- [ ] Controller updated for two-stage prediction
- [ ] Live testing with watch + game
- [ ] Performance monitored and iterated

## ğŸ”— References

- **GitHub Repo**: [v3-watch_SilksongController_25TPE](https://github.com/CarlKho-Minerva/v3-watch_SilksongController_25TPE)
- **Colab Template**: `notebooks/Silksong_Complete_Training_Colab.ipynb`
- **Demo Videos**: See README.md

## âœ¨ Summary

**What We Built**:
- âœ… Two-stage classification architecture
- âœ… Data organization system with class balancing
- âœ… Local SVM training (5-15 min, no GPU)
- âœ… Colab CNN-LSTM training (20-40 min, with GPU)
- âœ… Data augmentation tools
- âœ… Comprehensive documentation

**Current Status**:
- âœ… Binary classifier: 94% accuracy (production-ready)
- âš ï¸ Multi-class classifier: 57% accuracy (needs more data)
- âœ… Infrastructure complete for iterative improvement

**Next Steps**:
1. Collect more data (priority: jump, punch)
2. Apply augmentation
3. Try CNN-LSTM
4. Integrate into controller
5. Test and iterate

---

**Last Updated**: 2025-10-19  
**Contributors**: CarlKho-Minerva, GitHub Copilot  
**Status**: âœ… Complete and Ready for Use
