# Phase V: Continuous Motion Data Collection

## Overview

Phase V requires a **fundamentally different data collection approach** compared to Phase II. Instead of isolated gesture snippets, we record continuous motion with all gestures naturally mixed together.

---

## Why Continuous Motion?

### The Problem with Snippets (Phase II)

**Old Approach**:
```
Record: jump_01.csv (2.5s isolated jump)
Record: jump_02.csv (2.5s isolated jump)
...
Record: jump_40.csv (2.5s isolated jump)
```

**Issues**:
1. âŒ No transitions between gestures
2. âŒ Artificial start/stop in each clip
3. âŒ Model never sees realistic motion flow
4. âŒ Can't learn "walk â†’ jump â†’ walk" pattern

### The Solution: Continuous Recording

**New Approach**:
```
Record: session_01.csv (10 minutes of natural motion)
  Contains: walking, jumping, punching, turning mixed naturally
  With: Labels marking when each gesture occurs
```

**Advantages**:
1. âœ… Realistic transitions captured
2. âœ… Natural gesture flow
3. âœ… Context for LSTM to learn
4. âœ… Models actual gameplay patterns

---

## Recording Sessions

### Session Structure

**Duration**: 5-10 minutes per session  
**Goal**: Minimum 10 sessions = 50-100 minutes total data

**Natural Flow Example**:
```
0:00-0:15  â†’ Walk forward
0:15-0:16  â†’ Jump
0:16-0:30  â†’ Continue walking
0:30-0:31  â†’ Punch
0:31-0:45  â†’ Walk
0:45-0:50  â†’ Turn around
0:50-1:05  â†’ Walk other direction
1:05-1:06  â†’ Jump
... continues naturally for 5-10 minutes
```

### What to Perform

**During each session, mix these gestures naturally**:

1. **Walking** (70-80% of time)
   - Natural walking pace
   - Various speeds (slow, normal, fast)
   - Different arm swings

2. **Jumping** (5-10% of time)
   - Quick upward arm motion
   - Perform every 10-20 seconds
   - Mix with different speeds

3. **Punching** (5-10% of time)
   - Forward punch motion
   - Perform every 15-30 seconds
   - Mix strong and light punches

4. **Turning** (5-10% of time)
   - 180Â° wrist rotation
   - Perform every 20-40 seconds
   - Left and right turns

5. **Noise** (5-10% of time)
   - Scratching head
   - Adjusting watch
   - Random arm movements

---

## Data Collection Tool

### New Tool: Continuous Recorder

**File**: `src/continuous_data_collector.py`

**Features**:
- Records continuous sensor stream
- Keyboard shortcuts mark gesture boundaries
- Real-time visualization
- Auto-saves with timestamps
- Generates label files automatically

### Usage

**Step 1: Start Recording**
```bash
cd src
python continuous_data_collector.py --duration 600  # 10 minutes
```

**Step 2: Perform Motion + Mark Gestures**

While recording, press keys to mark gestures:
- **'j'** = Jump (marks next 0.3s as jump)
- **'p'** = Punch (marks next 0.3s as punch)
- **'t'** = Turn (marks next 0.5s as turn)
- **'n'** = Noise (marks next 1.0s as noise)
- **Everything else** = Walk (default state)

**Step 3: Review and Save**

Recording automatically saves:
- `data/continuous/session_01.csv` (sensor data)
- `data/continuous/session_01_labels.csv` (gesture labels)

### Recording Interface

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONTINUOUS GESTURE RECORDER                            â”‚
â”‚  Session: session_01                                     â”‚
â”‚  Duration: 03:24 / 10:00                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Current State: WALKING                                 â”‚
â”‚  Last Gesture: JUMP (03:23.5)                          â”‚
â”‚                                                         â”‚
â”‚  Sensor Data:                                           â”‚
â”‚    Accel: [2.3, -0.5, 9.8]                            â”‚
â”‚    Gyro:  [0.1, 0.0, -0.2]                            â”‚
â”‚    Rot:   [0.01, -0.02, 0.03, 0.99]                   â”‚
â”‚                                                         â”‚
â”‚  Gesture Counts This Session:                          â”‚
â”‚    Walk:  194 seconds                                   â”‚
â”‚    Jump:  12 events                                     â”‚
â”‚    Punch: 8 events                                      â”‚
â”‚    Turn:  6 events                                      â”‚
â”‚    Noise: 4 events                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CONTROLS:                                              â”‚
â”‚    j = Jump  |  p = Punch  |  t = Turn  |  n = Noise  â”‚
â”‚    q = Quit  |  s = Save Now                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Label File Format

### Structure

**File**: `session_01_labels.csv`

```csv
timestamp,gesture,duration
0.0,walk,15.2
15.2,jump,0.3
15.5,walk,14.6
30.1,punch,0.3
30.4,walk,14.6
45.0,turn,0.5
45.5,walk,14.5
60.0,jump,0.3
...
```

**Fields**:
- `timestamp`: Start time of gesture (seconds)
- `gesture`: Gesture name (walk, jump, punch, turn, noise)
- `duration`: How long gesture lasts (seconds)

### How Labels Are Used

**Training Process**:
```python
# Load sensor data
sensor_data = pd.read_csv('session_01.csv')
labels = pd.read_csv('session_01_labels.csv')

# For each timestep in sensor data
for t in range(len(sensor_data)):
    current_time = sensor_data.loc[t, 'timestamp']
    
    # Find which gesture is happening at this time
    for i, label in labels.iterrows():
        if label['timestamp'] <= current_time < label['timestamp'] + label['duration']:
            current_gesture = label['gesture']
            break
    
    # Train model to predict current_gesture at time t
    X = sensor_data[t-50:t]  # Last 1 second
    y = current_gesture
    train_model(X, y)
```

---

## Best Practices

### Recording Quality

**DO**:
- âœ… Perform gestures naturally as you would in gameplay
- âœ… Mix gesture speeds (fast, normal, slow)
- âœ… Include transitions between gestures
- âœ… Record in different environments
- âœ… Vary arm positions and orientations

**DON'T**:
- âŒ Perform gestures robotically
- âŒ Make every gesture exactly the same
- âŒ Record only perfect gestures
- âŒ Stop between each gesture
- âŒ Only record in one position

### Session Distribution

**Recommended**:
- Session 1-3: Normal walking pace, clear gestures
- Session 4-6: Faster pace, gameplay simulation
- Session 7-8: Different orientations, varied speeds
- Session 9-10: Edge cases, difficult transitions

### Gesture Timing

**Walk**: 
- Default state, happens most of time
- Don't mark unless specifically walking
- Natural arm swing

**Jump**:
- Mark right when you start upward motion
- Duration: 0.3 seconds
- Press 'j' at gesture start

**Punch**:
- Mark at start of forward motion
- Duration: 0.3 seconds
- Press 'p' at gesture start

**Turn**:
- Mark at start of rotation
- Duration: 0.5 seconds (slightly longer)
- Press 't' at gesture start

**Noise**:
- Any non-gesture motion
- Duration: 1.0 seconds
- Press 'n' when doing random motion

---

## Data Validation

### Automated Checks

After recording, validate data quality:

```bash
python src/validate_continuous_data.py --session session_01
```

**Checks**:
1. âœ… Sensor data continuity (no gaps)
2. âœ… Label coverage (all time labeled)
3. âœ… Gesture distribution (balanced classes)
4. âœ… Label overlap (no conflicts)
5. âœ… Minimum gesture counts

**Expected Output**:
```
Validating session_01...
  âœ… Sensor data: 30,000 samples (10 min @ 50Hz)
  âœ… No data gaps detected
  âœ… Labels cover 100% of recording
  âœ… No overlapping labels
  
Gesture Distribution:
  Walk:  450s (75%)
  Jump:  15s (2.5%)
  Punch: 12s (2%)
  Turn:  18s (3%)
  Noise: 105s (17.5%)
  
  âœ… Distribution acceptable
  âš ï¸  Jump slightly low (need 15+ jumps)
  
Overall: PASSED (minor warnings)
```

### Manual Review

**Visualize recording**:
```bash
python src/visualize_session.py --session session_01
```

**Shows**:
- Sensor readings over time
- Labeled gesture regions
- Detected anomalies
- Gesture transition points

**Example Plot**:
```
Acceleration Z-axis
  â–²
  â”‚        Jump!
  â”‚         /\
  â”‚        /  \
  â”‚  â”€â”€â”€â”€â”€/â”€â”€â”€â”€\â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Walk
  â”‚                  Punch!
  â”‚                    \/
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time
  
Labeled Regions:
  [Walk][Jump][Walk][Punch][Walk]...
```

---

## Dataset Requirements

### Minimum Data

**For Training**:
- **Total Sessions**: 10+ sessions
- **Total Duration**: 60+ minutes
- **Gesture Counts**:
  - Walk: 3000+ seconds
  - Jump: 60+ events (18+ seconds)
  - Punch: 50+ events (15+ seconds)
  - Turn: 40+ events (20+ seconds)
  - Noise: 300+ seconds

### Ideal Data

**For Production**:
- **Total Sessions**: 20+ sessions
- **Total Duration**: 120+ minutes
- **Multiple Users**: 3+ different people
- **Varied Conditions**: Different rooms, positions, speeds

---

## Data Augmentation

### Why Augmentation?

Even with continuous recording, we can increase dataset size:

**Techniques**:
1. **Time Warping**: Speed up/slow down slightly
2. **Noise Injection**: Add small random noise
3. **Rotation**: Rotate sensor axes (orientation change)
4. **Magnitude Scaling**: Scale sensor values slightly

**Implementation**:
```python
def augment_window(window):
    # Time warping
    window = time_warp(window, factor=0.9-1.1)
    
    # Add noise
    noise = np.random.normal(0, 0.1, window.shape)
    window = window + noise
    
    # Scale magnitude
    scale = np.random.uniform(0.9, 1.1)
    window = window * scale
    
    return window
```

**Benefits**:
- 2-3x more training data
- Better generalization
- Handles edge cases

---

## Troubleshooting

### Issue: Labels Don't Match Data

**Symptoms**: Model learns poorly, validation accuracy low

**Causes**:
- Marked gestures too early/late
- Duration too short/long
- Missed marking a gesture

**Solution**:
- Use visualization tool to review
- Re-record session if many errors
- Adjust timing in label file manually

### Issue: Imbalanced Gestures

**Symptoms**: Model always predicts "walk"

**Causes**:
- Too much walking (>90%)
- Too few action gestures

**Solution**:
- Perform more jumps/punches/turns per session
- Reduce walking time between gestures
- Use class weights during training

### Issue: Transitions Confuse Model

**Symptoms**: Poor accuracy at gesture boundaries

**Causes**:
- Abrupt transitions
- Overlapping labels
- Unclear gesture starts

**Solution**:
- Perform natural transitions
- Add small overlap in labels
- Train longer (LSTM learns transitions)

---

## Comparison: Phase II vs Phase V

| Aspect | Phase II (Snippets) | Phase V (Continuous) |
|--------|-------------------|---------------------|
| Recording | 40 isolated clips | 1 long recording |
| Duration per gesture | 2.5s | 0.3-1.0s |
| Transitions | None | Natural |
| Labels | Filenames | CSV file |
| Total time | ~90 minutes | ~60 minutes |
| Data realism | Low | High |
| LSTM training | Poor | Excellent |

---

## Next Steps

1. **Implement Recorder**: Create `continuous_data_collector.py`
2. **Record Sessions**: Collect 10+ sessions
3. **Validate Data**: Check quality and distribution
4. **Train Model**: Use continuous data with CNN/LSTM
5. **Evaluate**: Compare with Phase IV SVM

---

## Summary

Continuous motion data collection is the **foundation** of Phase V success. By capturing realistic gesture flow and transitions, we enable the CNN/LSTM model to learn true temporal patterns.

**Key Principles**:
- Natural motion beats isolated clips
- Transitions are as important as gestures
- Quality over quantity (but need minimum quantity)
- Validation prevents wasted training time

---

**Next**: See `CNN_LSTM_ARCHITECTURE.md` for model details  
**Status**: ğŸš§ Ready to implement recorder tool
