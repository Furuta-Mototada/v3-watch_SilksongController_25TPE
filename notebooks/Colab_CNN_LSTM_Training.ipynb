{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a26573",
   "metadata": {},
   "source": [
    "# Silksong Gesture Recognition - CNN/LSTM Training\n",
    "\n",
    "**Training for Hollow Knight: Silksong Voice-Controlled Watch Interface**\n",
    "\n",
    "This notebook trains a CNN/LSTM deep learning model for real-time gesture recognition.\n",
    "\n",
    "## Setup Requirements:\n",
    "1. ‚úÖ Enable GPU: Runtime > Change runtime type > GPU (T4 recommended)\n",
    "2. ‚úÖ Upload your data to Google Drive in: `My Drive/silksong_data/`\n",
    "3. ‚úÖ Each session folder should contain:\n",
    "   - `sensor_data.csv` (accelerometer + gyroscope data)\n",
    "   - `[session]_labels.csv` (gesture labels with timestamps)\n",
    "\n",
    "## Expected Training Time:\n",
    "- **With GPU (T4):** 20-40 minutes\n",
    "- **Without GPU (CPU):** 2-4 hours (not recommended)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86407884",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aceaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted!\")\n",
    "print(\"Your data should be in: /content/drive/MyDrive/silksong_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f164440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"\\n‚úÖ GPU is enabled! Training will be fast.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected. Training will be slow.\")\n",
    "    print(\"   Enable GPU: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f205130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe216dd",
   "metadata": {},
   "source": [
    "## 2. Configure Data Paths\n",
    "\n",
    "**Update this cell with your session folder names!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c942800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory in Google Drive\n",
    "DATA_DIR = '/content/drive/MyDrive/silksong_data'\n",
    "\n",
    "# List your session folders here\n",
    "SESSION_FOLDERS = [\n",
    "    '20251017_125600_session',\n",
    "    '20251017_135458_session',\n",
    "    '20251017_141539_session',\n",
    "    '20251017_143217_session',\n",
    "    '20251017_143627_session',\n",
    "]\n",
    "\n",
    "# Model configuration\n",
    "WINDOW_SIZE = 50  # 1 second at 50Hz sampling rate\n",
    "STRIDE = 25       # 50% overlap between windows\n",
    "NUM_FEATURES = 9  # 3 sensors √ó 3 axes (accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, etc.)\n",
    "\n",
    "# Gesture classes\n",
    "GESTURES = ['jump', 'punch', 'turn', 'walk', 'noise']\n",
    "NUM_CLASSES = len(GESTURES)\n",
    "\n",
    "print(f\"Configured {len(SESSION_FOLDERS)} sessions for training\")\n",
    "print(f\"Gestures: {GESTURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb93add",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session_data(session_folder):\n",
    "    \"\"\"Load sensor data and labels for one session\"\"\"\n",
    "    session_path = os.path.join(DATA_DIR, session_folder)\n",
    "\n",
    "    # Load sensor data\n",
    "    sensor_file = os.path.join(session_path, 'sensor_data.csv')\n",
    "    sensor_data = pd.read_csv(sensor_file)\n",
    "\n",
    "    # Load labels\n",
    "    labels_file = os.path.join(session_path, f'{session_folder}_labels.csv')\n",
    "    labels_data = pd.read_csv(labels_file)\n",
    "\n",
    "    return sensor_data, labels_data\n",
    "\n",
    "\n",
    "def create_label_vector(sensor_data, labels_data):\n",
    "    \"\"\"Create per-sample labels from segment labels\"\"\"\n",
    "    num_samples = len(sensor_data)\n",
    "    label_vector = np.full(num_samples, -1, dtype=int)\n",
    "\n",
    "    # Assuming 50Hz sampling rate\n",
    "    sample_rate = 50.0\n",
    "\n",
    "    for _, row in labels_data.iterrows():\n",
    "        start_time = row['timestamp']\n",
    "        duration = row['duration']\n",
    "        gesture = row['gesture']\n",
    "\n",
    "        if gesture not in GESTURES:\n",
    "            continue\n",
    "\n",
    "        gesture_idx = GESTURES.index(gesture)\n",
    "\n",
    "        # Convert time to sample indices\n",
    "        start_idx = int(start_time * sample_rate)\n",
    "        end_idx = int((start_time + duration) * sample_rate)\n",
    "\n",
    "        # Clip to valid range\n",
    "        start_idx = max(0, min(start_idx, num_samples))\n",
    "        end_idx = max(0, min(end_idx, num_samples))\n",
    "\n",
    "        label_vector[start_idx:end_idx] = gesture_idx\n",
    "\n",
    "    return label_vector\n",
    "\n",
    "\n",
    "def create_windows(sensor_data, labels, window_size, stride):\n",
    "    \"\"\"Create sliding windows from continuous data\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    num_samples = len(sensor_data)\n",
    "\n",
    "    for i in range(0, num_samples - window_size, stride):\n",
    "        window = sensor_data[i:i+window_size]\n",
    "        window_labels = labels[i:i+window_size]\n",
    "\n",
    "        # Skip if window contains unlabeled data\n",
    "        if np.any(window_labels == -1):\n",
    "            continue\n",
    "\n",
    "        # Use majority vote for window label\n",
    "        label = np.bincount(window_labels).argmax()\n",
    "\n",
    "        X.append(window)\n",
    "        y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process all sessions\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for session_folder in SESSION_FOLDERS:\n",
    "    print(f\"\\nProcessing {session_folder}...\")\n",
    "\n",
    "    try:\n",
    "        sensor_data, labels_data = load_session_data(session_folder)\n",
    "        print(f\"  Sensor samples: {len(sensor_data)}\")\n",
    "        print(f\"  Label segments: {len(labels_data)}\")\n",
    "\n",
    "        # Extract features (assume columns are: timestamp, accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, ...)\n",
    "        feature_cols = [col for col in sensor_data.columns if col != 'timestamp']\n",
    "        features = sensor_data[feature_cols].values\n",
    "\n",
    "        # Create per-sample labels\n",
    "        label_vector = create_label_vector(sensor_data, labels_data)\n",
    "\n",
    "        # Create sliding windows\n",
    "        X, y = create_windows(features, label_vector, WINDOW_SIZE, STRIDE)\n",
    "        print(f\"  Generated {len(X)} windows\")\n",
    "\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Combine all sessions\n",
    "if all_X:\n",
    "    X_combined = np.concatenate(all_X, axis=0)\n",
    "    y_combined = np.concatenate(all_y, axis=0)\n",
    "\n",
    "    print(f\"\\n‚úÖ Total training windows: {len(X_combined)}\")\n",
    "    print(f\"   Input shape: {X_combined.shape}\")\n",
    "    print(f\"   Labels shape: {y_combined.shape}\")\n",
    "\n",
    "    # Show class distribution\n",
    "    print(\"\\n   Class distribution:\")\n",
    "    for i, gesture in enumerate(GESTURES):\n",
    "        count = np.sum(y_combined == i)\n",
    "        percentage = count / len(y_combined) * 100\n",
    "        print(f\"     {gesture}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No data loaded! Check your data paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263dd63",
   "metadata": {},
   "source": [
    "## 4. Split Train/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "X_combined, y_combined = shuffle(X_combined, y_combined, random_state=42)\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_combined, y_combined, test_size=0.15, random_state=42, stratify=y_combined\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 of 0.85 ‚âà 0.15 overall\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {len(X_train)} samples ({len(X_train)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"Test set:       {len(X_test)} samples ({len(X_test)/len(X_combined)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73fe7c",
   "metadata": {},
   "source": [
    "## 5. Build CNN/LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7be364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Create CNN/LSTM architecture for gesture recognition\"\"\"\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        # CNN layers for feature extraction\n",
    "        layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        # LSTM layers for temporal modeling\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.LSTM(64),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Dense layers for classification\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "input_shape = (WINDOW_SIZE, NUM_FEATURES)\n",
    "model = create_cnn_lstm_model(input_shape, NUM_CLASSES)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7ff38",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ff04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b090c",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Train')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], label='Train')\n",
    "ax2.plot(history.history['val_loss'], label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nüìä Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_classes, target_names=GESTURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f97707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=GESTURES, yticklabels=GESTURES)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b478209",
   "metadata": {},
   "source": [
    "## 8. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "model_save_path = '/content/drive/MyDrive/silksong_data/cnn_lstm_gesture.h5'\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "print(\"\\nDownload this file to your local project and place it in the 'models/' directory\")\n",
    "print(\"Then run: python src/udp_listener_v3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e889e1",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download the trained model:**\n",
    "   - Right-click on the file in Google Drive: `silksong_data/cnn_lstm_gesture.h5`\n",
    "   - Download to your local machine\n",
    "\n",
    "2. **Place model in your project:**\n",
    "   ```bash\n",
    "   # Move to your project's models directory\n",
    "   mv ~/Downloads/cnn_lstm_gesture.h5 /path/to/project/models/\n",
    "   ```\n",
    "\n",
    "3. **Test real-time recognition:**\n",
    "   ```bash\n",
    "   cd src\n",
    "   python udp_listener_v3.py\n",
    "   ```\n",
    "\n",
    "4. **Expected performance:**\n",
    "   - Latency: 10-30ms per prediction\n",
    "   - Accuracy: 90-98%\n",
    "   - Much faster than Phase IV SVM model!\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the documentation in `docs/Phase_V/README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
