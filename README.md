# Silksong Motion Controller - Machine Learning Pipeline (First Draft)

**Course**: CS156 Machine Learning  
**Project**: Motion Gesture Recognition for Game Control  
**Hardware**: Pixel Watch (Wear OS) + Python Controller

Control Hollow Knight: Silksong using motion gestures from a smartwatch. This project demonstrates a complete machine learning pipeline from data collection through model deployment, using real-time IMU sensor data streamed over UDP.

Building on top of [V2](https://github.com/CarlKho-Minerva/v2_SilksongController_25TPE)

---

**‚ö†Ô∏è Academic Context**: This is a **first draft** machine learning pipeline for educational purposes. The focus is on demonstrating ML fundamentals (data collection, feature engineering, model training, deployment) rather than achieving production-ready performance. Second and final draft iterations will follow.

---

## üÜï NEW: Two-Stage Classification Training Pipeline

**Ready to train gesture recognition models?** We've added a complete training pipeline with organized data and two-stage classification:

üìä **[Data Organization Guide](docs/DATA_ORGANIZATION_GUIDE.md)** - Complete workflow from data collection to training

üéØ **[Training Results Summary](docs/TRAINING_RESULTS_SUMMARY.md)** - Current model performance and recommendations

üß™ **[SVM Local Training](notebooks/SVM_Local_Training.ipynb)** - Fast local training (5-15 min, no GPU needed)

‚òÅÔ∏è **[Colab Training](notebooks/Silksong_Complete_Training_Colab.ipynb)** - Deep learning training with CNN-LSTM

### Two-Stage Architecture
1. **Binary Classifier**: Walking vs Not-Walking (94% accuracy ‚úÖ)
2. **Multi-class Classifier**: Jump, Punch, Turn, Idle (57% accuracy - needs more data)

### Quick Start Training
```bash
# 1. Organize your collected data
python src/organize_training_data.py --input data/button_collected --output data/organized_training

# 2. Train models locally (SVM)
python notebooks/SVM_Local_Training.py

# 3. Models saved to models/ directory
ls models/gesture_classifier_*.pkl
```

---

## üÜï Button Data Collection with Dashboard

**Having trouble collecting data?** We've added a comprehensive data collection system with real-time verification:

üìä **[Quick Start Guide](DASHBOARD_QUICK_START.md)** - Get started with the new dashboard in 5 minutes

üîß **[Troubleshooting Guide](DATA_COLLECTION_VERIFICATION.md)** - Step-by-step verification and problem solving

ü§ù **[App Coordination Guide](ANDROID_COORDINATION_GUIDE.md)** - How Watch and Phone apps work together

### Quick Commands

```bash
# 1. COLLECT DATA - Start the real-time dashboard (RECOMMENDED)
cd src/phase_vi_button_collection
python data_collection_dashboard.py

# 2. ORGANIZE DATA - Prepare for training
cd src
python organize_training_data.py --input ../data/button_collected --output ../data/organized_training

# 3. TRAIN MODELS - Local SVM training (5-15 min)
python notebooks/SVM_Local_Training.py
# OR use Colab for CNN-LSTM: notebooks/Silksong_Complete_Training_Colab.ipynb

# 4. VERIFY DATA - Check quality after collection
cd src/shared_utils
python inspect_csv_data.py ../../data/button_collected/*.csv

# 5. RUN CONTROLLER - Use trained models
cd src/phase_iv_ml_controller
python udp_listener.py

# Optional: Augment minority classes
cd src
python data_augmentation.py --input ../data/organized_training/multiclass_classification --target-samples 50
```

**Key Features**:
- ‚úÖ Real-time sensor data visualization
- ‚úÖ Live connection status for Watch and Phone apps
- ‚úÖ Data rate monitoring
- ‚úÖ CSV quality inspector
- ‚úÖ Comprehensive troubleshooting

---

## üìπ Demo Videos

**Watch Data Transmission**: [Loom Video](https://www.loom.com/share/175721940a354cb98fe0ec2a13e2bddf) - NSD discovery and UDP streaming  
**SVM Controller Demo**: [Loom Video](https://www.loom.com/share/dfb0398e038c409084696484e159a588) - Phase III real-time gesture recognition  
**Voice-Labeled Data Collection**: [Loom Video](https://www.loom.com/share/db304cbfea1d4fa4914256f097d4a166) - Phase V collection process

## üéì Learning Objectives

This project demonstrates:

1. **Data Collection**: Voice-labeled continuous recording with WhisperX post-processing
2. **Feature Engineering**: Time-series features from IMU data (accel, gyro, rotation)
3. **Model Training**: SVM (local) and CNN-LSTM (Colab) architectures
4. **Real-time Deployment**: Multi-threaded Python controller with <500ms latency
5. **Iteration & Design Thinking**: Documented exploration of data collection methods

## üéÆ Key Features

- **Smartwatch Sensors**: Accelerometer, gyroscope, rotation vector, step detector
- **Automatic Discovery**: NSD (Network Service Discovery) - no manual IP setup
- **Real-time UDP Streaming**: ~50Hz sensor data transmission
- **ML-Powered Gestures**: Walk, jump, attack, turn detection via trained models
- **Voice-Labeled Training Data**: Natural gameplay with retrospective labeling

## üìÅ Project Structure

```text
v3-watch_SilksongController_25TPE/
‚îú‚îÄ‚îÄ Android/                         # Wear OS app (streams sensor data via UDP)
‚îÇ   ‚îî‚îÄ‚îÄ app/src/main/java/com/cvk/silksongcontroller/
‚îÇ       ‚îî‚îÄ‚îÄ MainActivity.kt          # Sensor streaming + NSD discovery
‚îú‚îÄ‚îÄ src/                             # Python ML pipeline (organized by phase)
‚îÇ   ‚îú‚îÄ‚îÄ phase_ii_manual_collection/  # Button-based data collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ button_data_collector.py # Simplified one-button UI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_collector.py        # Original guided collector
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_collection_dashboard.py # Real-time monitoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Phase II documentation
‚îÇ   ‚îú‚îÄ‚îÄ phase_iv_ml_controller/      # Real-time ML controller
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ udp_listener.py          # Main ML controller (multi-threaded)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ udp_listener_v3.py       # CNN-LSTM version
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calibrate.py             # Threshold calibration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Phase IV documentation
‚îÇ   ‚îú‚îÄ‚îÄ phase_v_voice_collection/    # Voice-labeled data collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ continuous_data_collector.py # Natural gameplay recording
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ whisperx_transcribe.py   # Audio transcription
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ align_voice_labels.py    # Label alignment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Phase V documentation
‚îÇ   ‚îú‚îÄ‚îÄ shared_utils/                # Common utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network_utils.py         # NSD + UDP helpers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py     # Feature engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_connection.py       # Connection testing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inspect_csv_data.py      # Data validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Utilities documentation
‚îÇ   ‚îú‚îÄ‚îÄ models/                      # ML model code
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cnn_lstm_model.py        # Deep learning architecture
‚îÇ   ‚îî‚îÄ‚îÄ config.json                  # Runtime configuration
‚îú‚îÄ‚îÄ data/                            # Training data (organized by phase)
‚îÇ   ‚îú‚îÄ‚îÄ phase_ii_button_collected/   # Button-collected samples
‚îÇ   ‚îî‚îÄ‚îÄ phase_v_continuous/          # Voice-labeled sessions
‚îÇ       ‚îú‚îÄ‚îÄ continuous/              # Active sessions
‚îÇ       ‚îî‚îÄ‚îÄ archive/                 # Historical data + scripts
‚îú‚îÄ‚îÄ notebooks/                       # Training notebooks
‚îÇ   ‚îú‚îÄ‚îÄ Silksong_Complete_Training_Colab.ipynb  # All-in-one training
‚îÇ   ‚îú‚îÄ‚îÄ watson_Colab_CNN_LSTM_Training.ipynb    # Original CNN-LSTM
‚îÇ   ‚îî‚îÄ‚îÄ archive/                     # Previous experiments
‚îú‚îÄ‚îÄ models/                          # Trained model artifacts (generated)
‚îÇ   ‚îú‚îÄ‚îÄ README.md                    # Model documentation
‚îÇ   ‚îî‚îÄ‚îÄ archive/                     # Previous model versions
‚îú‚îÄ‚îÄ docs/                            # Documentation organized by phase
‚îÇ   ‚îú‚îÄ‚îÄ Phase_II/                    # Manual data collection
‚îÇ   ‚îú‚îÄ‚îÄ Phase_III/                   # SVM training (local)
‚îÇ   ‚îú‚îÄ‚îÄ Phase_IV/                    # Multi-threaded ML controller
‚îÇ   ‚îú‚îÄ‚îÄ Phase_V/                     # Voice-labeled collection + WhisperX
‚îÇ   ‚îú‚îÄ‚îÄ COLAB_TRAINING_GUIDE.md      # Google Colab training guide
‚îÇ   ‚îú‚îÄ‚îÄ CHRONOLOGICAL_NARRATIVE.md   # Project history
‚îÇ   ‚îú‚îÄ‚îÄ SESSION_BUTTON_PIVOT_NARRATIVE.md # Recent improvements
‚îÇ   ‚îî‚îÄ‚îÄ archive/                     # Historical docs
‚îî‚îÄ‚îÄ requirements.txt                 # Python dependencies
```

## üöÄ Quick Start

### Prerequisites

- **Hardware**: Pixel Watch (Wear OS)
- **Software**:
  - Python 3.8+ with pip
  - Android Studio (for watch app)
  - Google Colab (for CNN-LSTM training)

### Installation

```bash
# 1. Clone repository
git clone https://github.com/CarlKho-Minerva/v3-watch_SilksongController_25TPE.git
cd v3-watch_SilksongController_25TPE

# 2. Install Python dependencies
pip install -r requirements.txt

# 3. Build Android app (see Android Studio instructions in installer/INSTALLATION_GUIDE.md)

# 4. Collect training data
cd src/phase_ii_manual_collection
python button_data_collector.py

# 5. Train model on Google Colab (see docs/COLAB_TRAINING_GUIDE.md)
# Upload data to Google Drive and use notebooks/Silksong_Complete_Training_Colab.ipynb

# 6. Run the controller with trained models
cd src/phase_iv_ml_controller
python udp_listener.py
```

The watch app will automatically discover your computer via NSD - no manual IP configuration needed!

## üìä Machine Learning Pipeline Overview

This project evolved through multiple phases, demonstrating different approaches to gesture recognition:

### Phase II: Manual Data Collection ‚úÖ
Button-based and guided recording system for collecting labeled gesture samples.

**Tools**: 
- `src/phase_ii_manual_collection/button_data_collector.py` - Simplified one-button UI
- `src/phase_ii_manual_collection/data_collection_dashboard.py` - Real-time monitoring
- `src/phase_ii_manual_collection/data_collector.py` - Original guided collector

**Output**: CSV files in `data/phase_ii_button_collected/`  
**Documentation**: `src/phase_ii_manual_collection/README.md`

### Phase III: SVM Classifier ‚úÖ
Traditional ML pipeline using hand-engineered features (~60 features from time/frequency domains) with SVM classification.

**Training**: Google Colab notebook with SVM option  
**Notebook**: `notebooks/Silksong_Complete_Training_Colab.ipynb`  
**Features**: Time-domain stats, FFT features, magnitude calculations  
**Performance**: 85-95% accuracy on test set  
**Guide**: `docs/COLAB_TRAINING_GUIDE.md`

### Phase IV: Multi-Threaded Controller ‚úÖ **(Current Production)**
Real-time ML deployment with decoupled architecture for low-latency performance.

**Architecture**: Collector ‚Üí Predictor ‚Üí Actor (three threads with queues)  
**Latency**: <500ms using 0.3s micro-windows  
**Confidence Gating**: 5 consecutive predictions for state changes  
**Code**: `src/phase_iv_ml_controller/udp_listener.py`

See `src/phase_iv_ml_controller/README.md` for architecture details.

### Phase V: Voice-Labeled Data Collection ‚úÖ **(Advanced)**
Natural gameplay recording with voice commands, retrospectively processed using WhisperX for word-level timestamp alignment.

**Tools**:
- `src/phase_v_voice_collection/continuous_data_collector.py` - Natural gameplay recording
- `src/phase_v_voice_collection/whisperx_transcribe.py` - Audio transcription
- `src/phase_v_voice_collection/align_voice_labels.py` - Label alignment

**Workflow**:
```bash
# 1. Record session (natural gameplay + voice labels)
cd src/phase_v_voice_collection
python continuous_data_collector.py --duration 600 --session gameplay_01

# 2. Post-process with WhisperX
python whisperx_transcribe.py --audio ../../data/phase_v_continuous/SESSION/audio_16k.wav

# 3. Align labels
python align_voice_labels.py --session_dir ../../data/phase_v_continuous/SESSION/
```

**Advantages**: Hands-free, natural motion, word-level timestamps, captures transitions  
**Use Case**: Training CNN-LSTM models with large datasets  
**Documentation**: `src/phase_v_voice_collection/README.md`

### Phase VI: Button Data Collection *(Proposed)*
Alternative data collection approach using button-grid Android app for more controlled, precise labeling.

üìÑ **`docs/BUTTON_DATA_COLLECTION_PROTOCOL.md`** - Complete implementation specification
- Press-and-hold interaction model with exact timestamps
- 2x3 button grid layout with real-time count display
- Dual classifier architecture (walk/idle vs. actions)
- Three-stage streaming pipeline (Watch ‚Üí Phone ‚Üí MacBook)
- Data integrity and noise handling strategies
- MVP implementation guide and testing protocol

üìÑ **`docs/ALTERNATIVE_DATA_COLLECTION_BRAINSTORM.md`** - Design thinking and expert analysis
- Analysis of current data quality issues
- Expert panel discussion (Ng, Li, Jordan, Norman, Ries)
- Trade-offs: organic vs. controlled data collection
- Recommendations for academic pipeline drafts

## üíæ Current Data Status

**Collected Sessions**: 5 voice-labeled gameplay sessions (archived in `src/data/archive/`)
- Session duration: 5-10 minutes each
- Gestures: walk, punch, jump, turn, idle
- Format: CSV sensor data + audio recordings + WhisperX transcriptions

**Known Issues**:
- Labels are noisy (voice-motion coordination challenges)
- Walk vs. non-walk classification has unclear boundaries
- Action transitions are difficult to isolate cleanly

**Recommendation**: Use existing data to demonstrate the complete pipeline in first draft, acknowledge data quality limitations in write-up. For second draft, consider implementing button-grid collection method (see brainstorming doc).

## üìö Documentation Structure

- **`docs/Phase_II/`** - Manual data collection (archived approach)
- **`docs/Phase_III/`** - SVM training pipeline (archived)
- **`docs/Phase_IV/`** - Multi-threaded controller architecture (current)
- **`docs/Phase_V/`** - Voice-labeled data collection (current)
- **`docs/BUTTON_DATA_COLLECTION_PROTOCOL.md`** - Button grid protocol specification (proposed)
- **`docs/ALTERNATIVE_DATA_COLLECTION_BRAINSTORM.md`** - Design exploration and expert analysis
- **`docs/CHRONOLOGICAL_NARRATIVE.md`** - Complete project history
- **`docs/archive/`** - Historical troubleshooting guides and training docs

## üîß Development Notes

**Android App**: 
- Package: `com.cvk.silksongcontroller`
- Main code: `Android/app/src/main/java/com/cvk/silksongcontroller/MainActivity.kt`
- Streams sensor data over UDP with NSD discovery

**Python Controller**:
- Entry point: `src/udp_listener.py` (Phase IV multi-threaded architecture)
- Data collection: `src/continuous_data_collector.py` (Phase V voice labeling)
- Feature extraction: `src/feature_extractor.py` (~60 engineered features)

**Training**:
- Local SVM: `notebooks/archive/CS156_Silksong_Watch.ipynb`
- Colab CNN-LSTM: `notebooks/watson_Colab_CNN_LSTM_Training.ipynb`

## üéì Academic Context

This project is part of CS156 Machine Learning coursework, demonstrating:
- Complete ML pipeline implementation
- Design thinking and iteration (documented in brainstorming doc)
- Trade-off analysis (data quality vs. collection methodology)
- Real-world deployment considerations

**Note**: This is a **first draft** pipeline. Emphasis is on demonstrating understanding of ML fundamentals rather than achieving state-of-the-art performance. Future drafts will iterate on data quality and model architecture.

## üìù License

MIT License - see LICENSE file for details

## üôè Acknowledgments

Built on [V2 SilksongController](https://github.com/CarlKho-Minerva/v2_SilksongController_25TPE) with significant enhancements:
- Pixel Watch integration with NSD discovery
- ML-powered gesture recognition (SVM and CNN-LSTM)
- Voice-labeled data collection pipeline
- Multi-threaded real-time controller architecture
