 Thouroughly summarize and cite why u chose to include list from text im psating
"Let's grab Lena. All right, so Maria, everybody take a look at these learning objectives. These are the four big ones. You can ignore development for now. That's kind of bonus points. Maria, give me an overview of what you think goes into ML explanation. What would you look for if you were evaluating ML explanation? I think I would try to see that the student has a proper understanding of how different parts and pieces of the algorithm work together and also contextualizing them in a given data problem. For example, if you have a classification problem, does it make sense that I chose logistic regression instead of like linear regression. And then I am able to, if I am able to explain how logistic regression works, how it draws its boundary, how it classifies, I don't know, et cetera, et cetera. - So far so good. When you look at their assignment, what kinds of things are you looking for? Like what stuff would you see in an assignment that you'd be like, oh, that's good explanation? - I don't know, I guess like a clearly following step by step process. So not just jumping and skipping over three steps altogether. Also, a lot of justification. It's easy to tell an AI, oh, what model should I use? And that's it. But I think it would be important to kind of see a clear explanation and justification of like, oh, maybe I also explored this other model or oh, I thought of taking this approach, but then I realized that maybe that wasn't the best idea, et cetera. It's also kind of reflecting on the process of building the algorithm and the pipeline. Awesome. Yeah. So outlines, justification, step-by-step stuff, right? Descriptions of why different choices are made. The fact that different choices are made is actually part of explanation. It's like, oh, I did this choice instead of that choice. Or I did these two choices and here's the comparison. Those sorts of things, definitely good explanation. Okay, Gisela, tell me about code. How would you examine the code of an assignment to understand whether someone had done a good job? I think it has to first be in line with the ML explanation part. So the explanation that Maria just described, how the model should solve a certain problem, should also be present in the code, and we should see the output as we expected. Yep. The tree level would be using... Sorry, you're cutting in and out a little bit. Go ahead. Sorry about that. My internet is not the best one. I would also say that a tree level would be using the libraries that we got to sensor flow and using the functions that we were already provided. But I would guess the level would be actually dissecting those functions that are ready into our own code and trying to code it from scratch. Because I feel like that's what we were doing pre-class work. So whenever we were doing a function, it was a tree level when you were using the libraries, but if you actually wanted to go down and understand what that function is doing, you would get for if you called the function from scratch. So yeah, I think that's the... More or less. What I would do for it. Yeah, so does the code do what you say it does? Does it work and run? Does it produce good results? And is it readable? Can I understand what's going on? And then kind of like how much effort went into Are you pulling a bunch of libraries? Did you write everything from scratch? One of the things I recommend is if you want to try the writing from scratch approach, first do the everything from libraries approach. Because one of the things you'll need to do in the from scratch approach is compare the output of your from scratch to the output of a library to show that they match up. Otherwise, how do I know that you're doing the right thing? So once you've got your pipeline built not from scratch, If you want to try the actual from scratch thing, you're also in safer hands that way. You've got a working version of it, a version that's going to do well. And now you can do the stretch goals. Okay, Andreas, how about math? How do you think I would evaluate, I would go about evaluating the ML map of your pipeline? Can you hear me? Yeah, so I think one way to go about this is like how they use, like how they're able to like relate the modics they use in a way that makes sense for us, like to relate them with some of like the mathematical explanation or like a modules that we know of so like for example we could like evaluate them on what kind of like cost function they use and like how they were able to like explain the math behind that because we could use like a different alternatives in this instance, maybe it could be like a cross entropy or like a mean square here. And yeah, I think we could also like go to how they decided to use that specific module. So like the math behind it, it could like relate into different things. these things, I agree with all these things, what are some things that I'm looking for in the paper? This is kind of similar to the question I asked Maria, like what are some things you would expect to see in the paper that would tell you like oh this person engaged with the mathematics in this model, like you gave the example of a loss function or something like that, what are some things that you would expect to see if someone was going to unpack a loss function? - Fill you, Andreas. - Yeah, we could see how they calculated the loss, if we want to go to the specifics, like the residual calculation, or we could also use how they evaluated the way they update the weights for the backpropagation. This is a great division. So on the one hand, one thing I would expect is equations. Show me the actual, did you put equations in Markdown? Did you typeset your equations? If so, that gives me a lot of information about what you're, I can actually read your equation and say, oh, okay, this is what this thing is doing. If you introduce something like, suppose you're going to use something we haven't yet covered in class, like categorical cross entropy to do one of the explanations. You're going to need to explain that, and you're going to need to write down the equation for it so that I know that you know what you're talking about. Enrius also gave another example where sometimes this could be embedded in the code as well. And so I am reading the code to see, especially if you write from scratch code, I am reading the code to see like, okay, sometimes I can follow your steps and your logic in the code. That's fine with me as well. It doesn't have to always be that. One thing I like a lot are pseudo algorithms. So it's like a little pseudo code that has kind of variables in it that say like, okay, we're going to run through this loop. We're going to do these update steps. We're going to compare these two things. if you write out a little block of a pseudo algorithm. I'm pretty happy with that as well. So I'm expecting to see those things when I read through. And Lina, how about you? ML flexibility. What am I looking for when I talk about flexibility? When I looked over, it feels like it's grading whether I should get a five or not. So five is like, yes, I should extremely get a five. Four is like, yes, I'm more likely to get a five. But yeah, I think it's about just like if you have, like I feel like in machine learning specifically, sometimes your results are not going to be making sense, or sometimes they're not going to be the best thing you're looking for. So I feel like this is about seeing how you adapt your approach and your data and the way that you handle what you're doing to actually get, like, not necessarily, like, a good result, but just a result that, like, I guess makes sense. And, yeah, I guess. So for flexibility, everyone, I'm specifically grading you on using stuff. and I will score this for sure, I'm specifically grading you on using things that we did not cover in class. Machine learning is too big a topic for you to learn in one semester. And if you reach the end of this class and the only thing you have learned is the stuff that I have taught you, I have failed you as an instructor because you will not be able to do anything else. And everything else is the important part. And so in every assignment, I expect you to use some stuff we did not cover. And if you don't use stuff we did not cover, you will not be filling the bill of the assignment. Right? That means go and use a model that we haven't talked about yet. Right? Go and use a loss function that we haven't talked about. Explain why you used it. Go compute a metric that we haven't used. Why? Why did you use that one? Explain it. How does it work? Write down the equation for it. That is what flexibility is about. And I expect it in every assignment. Now, in the first assignment, It's actually easy to get good flexibility scores because everything in a, like you can just look at the class syllabus and scroll down here to the second unit and just steal some stuff from down here. Totally counts. We haven't covered it yet. And that's all I'm checking. I'm only checking if we've covered it yet. So it's easier on the early assignments to cover stuff that we didn't cover in class because we haven't covered it yet. Later on, you'll have to go and be more creative and explore more stuff outside of the class. A lot of this will come naturally, especially for those of you who are doing things like text, who are doing things like audio. because a lot of the preprocessing that goes into that is stuff we haven't covered in class, and all of that counts. A lot of this stuff is already kind of baked into your pipeline. If you have to go out and struggle with a data type that we don't have, that we've mostly done numerical data, if you've got to struggle with images in the first class, oh, man, that's hard. But if you get through it, hey, that's some flexibility points right there. That is what I'm looking for. Does that make sense, everybody? And with that in mind, here's Ingrid's assignment. Okay, so this is something I'm very, very pleased with. One of my students donated her assignment to the class so that we can go over it. And I particularly like this assignment as a thing to live grade through, because this is an assignment that, on the one hand, there are some parts of this I really love, and there are some other parts of this that I think struggle a bit. And I really want to show what -- I really want to give you the lens of looking at an assignment as an evaluator, rather than looking at an assignment as someone who is creating one, so that you can understand what goes into evaluation and thereby make no mistakes when you create your thing. Okay, so this is an assignment from Ingrid, and it happened a couple years ago. She had this problem. Can machine learning tell Allison and Ingrid apart? Ever since the first day at Minerva, people have been struggling to tell me and Allison apart. Fair enough, we're both awesome, strong, independent women, but are we really that similar in appearance? students, staff, strangers, and even Ben Nelson has mistaken us for each other and we absolutely love it. We've made it our brand at this point. We keep people on their toes, but it's matching clothes, dyeing our hair the same way and other shenanigans. So the question is, are computers better at telling us apart than humans? Google Photos sometimes has a hard time, but it asks same or different person, but that pops up with other people as well. So do we look the same to a computer? Okay. Love this. This is a great framing. When I sit down to look at an assignment, this is the first thing that I look for. what the hell is going on with this assignment? And right up front here, she's told me what it is. It's easy to understand. It's fun. It made me laugh. So far, so good. Great, great start. We're off to a great start. I know what's going on. So the second thing I immediately think is like, okay, I start to generate some hypotheses about how you would go about doing this with machine learning. And I start thinking, okay, so she's going to have to pick out faces, probably. She's going to have to do comparisons. She's going to have to compute a couple of different models. She's probably looking at a classifier of some sort, right? probably a couple of classifiers. All right, reasonable idea. Let's try to figure out what's going on. Okay, so then I jump down here, and I'm reading through here, and she says, "To find the faces, we use a hard cascade detection with OpenCV. What do I immediately think of when I hear hard cascade detecting with OpenCV? Which of my four learning objectives?" Yeah, Finn. Flexibility, right? Like, we haven't talked about it. Exactly. We haven't talked about this in class yet. So I say, oh, great. There's some flexibility. Exactly what I'm looking for. Okay. Wonderful. The second thing she says is the CNN would probably do a better job of it, but OpenCD has pre-trained space models, so I'm going to try this one first. That's a great comparison. That's saying, okay, I'm doing this and not that. I'd probably do a CNN for assignment too. I like to start thinking about stuff like that. She says, we can use any classifier. I'm going to use a K&N. We never actually covered this model in class. Technically, this is in one of your pre-class works. It's just a thing of like, oh, you could go try this canon. But I still, it's a little bit of a reach here, so that's okay. It takes training data, there's feature vectors and class labels. Okay, great. Okay, so far so good. Now I'm going to look at the code. I'm going to see how this is actually implemented. Read through this. Okay, great. She's getting packages. Okay, this is importing the images. Okay, she's creating the cascade as an object here. Great. Excellent. Wonderful. I see this. What learning objective do I immediately think of when I see this very helpful picture? Yeah, Maria. I would guess maybe ML explanation, because she can use it to explain how the code worked. Yeah, exactly. Look, there's a thousand words right here, Maria. We have an expression of pictures worth a thousand words, right? Visualizations and pictures and just visualizing of the raw data is often a really critical part of explanation. It is extremely valuable to have these sorts of things that explain, you know, I look at this picture, I immediately see what the problem is. I understand why it's hard to tell these two apart. I understand how the model is capturing their faces. All of this is done with one image. Really, really, really useful. So that's something that I'm immediately thinking of. Keep going, imports the images. She's going to iterate through them. She's going to try classifying them. Okay, great. Next one. Okay. Now it's going to do the classification with KNNs. And now I go to this page. Okay. What's going on here? Yeah, Lena. Is it like a visual for the true positives and true negatives and the false positive and the false negatives? No. This is our confusability matrix, and it's the confusability matrix between her and Ingrid, right? And it looks like the model is somewhat confused. I think it's a little above chance here. Basically, this eats this. This eats this, and we've still got 2.75, so I think it's a little above chance, but it does seem kind of confused. Yeah, Grant. I would also say I'm just kind of confused about a lot of things that's going on. Like, why do we have nine different confusion matrix? What is the score on the top? Yeah, just in general. Also, the color map is kind of weird. Like, why are you using CMAP to color that? Right, right. So the first thing I would do at this point, Grant, when I see something like this, I'm like, wait, wait, what the hell? Why are there nine? I'd go back, I'd look through this a little bit more closely. And it's like, oh, okay. It turns out these are all of the splits. She's doing cross-validation. She's sorting her data every different way she can and doing one confusability matrix for every leave one. I guess there's eight here, so she's leaving eight out, and she's running this on the eight she's left out and training it on all the rest. And so every possible section of eight that's left out, and it's like these are the different confusability matrices. It's like, okay, that makes sense. All of this is very confusing, though, But then, Grant, I turn the page. What's going on here? Yeah, Lena. Yeah, when I read it, I saw that she tested her face with another friend called Paul to make sure that the classifier is actually working. it's actually classifying two different people. So the problem is with the two people looking so similar and not that the model is not doing this job. So what I do when I see a page like this, great, exactly correct, Lena. So what I do when I see this is I go, oh yeah, oh yeah. It's pretty good at telling her and Paul apart. And it's pretty shit at telling her and Allison apart. She really does look like Allison. Even the robots think so. Right? That's really nice. This comparison is so valuable, right? This is the sort of thing I'm looking for. I'm like, tell me something. Did it work? Oh, I can look at these two pages and be like, yeah, it kind of worked, right? This is the sort of thing that I'm really looking for. It's like a really good payload for an assignment like this. I agree with Grant. It's a little bit weirdly presented in like having all of the different data things. I also think it's a little data poor, only like six samples of Paul and eight samples of Ingrid, or of Allison, make it kind of, see what I did there? Make it kind of hard to tell what is, what's going on. But the overall comparison is really, really valuable. A comparison like this is so critical to understanding what's going on. Right, and then I wrap it up. Okay, here's your references. Here's your little bit at the end, a little bit of a distinction. Solid, solid, solid. Okay. So now, having read and understood the paper, that's the first thing I always do. I go back to the beginning, and I think about my four learning objectives. So, go through them. How would you score each of these? Pick any one you want. Yeah, I'll start with flexibility. And I'll probably score five for the reach of going to more models like OpenCV and comparing them with the ones in class. I don't think she's using only models from outside of class, but also like she's comparing and contrasting to the ones that were in the class. - Yep. And for that, I would give a three. I would say this is about what I expect, right? Yeah, I agree. This is a good use of flexibility. She used flexibility on this assignment. She used as much flexibility as I expected, about three. Yeah. All right, Gisela. I was going to mention about ML Math. Even though some of her functions have variables as parameters, there's not much of an explanation of how the model that she chose uses mathematical. There's no mathematical derivation whatsoever, so there's a 2 on it. I agree. This seems to me like mostly an oversight. She does a little bit of an explanation of how KNN works, but then she doesn't actually put in, like, she doesn't actually show me, like, okay, here's how KNN computes, you know, the probability of being in this cluster, right? She just talks about it in vague terms about vectors and stuff like that. She talks about a hard cascade, but she doesn't tell me what a hard cascade is. And this is especially important because that's not something we covered in class. If it's something we covered in class, I could understand, like, oh, well, just like we covered on this day, you kind of use referencing language, but I don't know what this is, right? You can't assume that I know what this is. Unpack it for me, Show me the equations. And so on this, Giselle, I would give a one. I think this is just kind of a miss. Like it's not present. I don't want it to be harsh. Sorry. I'm hanging, Judge, Giselle. When I'm looking at this, the other thing that I would highlight here, Giselle, like you mentioned in her code, because she uses a bunch of packages, there's not a lot of safety net here. If she had written some stuff from scratch or if she had written, like if she was using a little bit more in her code things that like unpacked and like computed these vectors or something like this manually, I would be able to go in there and look at her code and say, oh, here's the variable she's using. Here's the computations that are actually happening. I can give some credit here because it's represented in the code. But because everything is a package, there's not a lot of safety net here. I will make a very similar point about any kind of LLM usage did you do here either in your code or in your text? It's a risky proposition because you don't have much safety net if you don't nail it. And since you used an LLM, you don't know if you nailed it. So I generally don't... I'm not too worried about students using LLMs for some of these things, but what I will say is they tend to result in a lot of twos and occasional ones because it's very hard for me to give back any credit. They get a floor under you. They'll get a function if you need to podge a function together. But damn, you better be sure that you unpack the equation for it in the math section. Otherwise, it's going to be hard for me to understand what the hell happened. Okay, how about explanation and code? Yeah, Yaroslav. I think for the code, I would say it's a string. It's pretty simply written. It's easy to follow. It's fairly commented. And when you were confused about the confusability matrix, so we're able to go back and see what's happening. So easy to unpack as well. I am mostly in agreement here. This code for me is kind of in between a 2 and a 3. I think that the code is probably the strongest part of the assignment. It works. It does the thing she wants. There are a few issues that I have with it, in mostly that it's very package heavy. And so I understand what the packages are doing, but I need to go and actually read the documentation to understand what is going on here in the classifier. The code doesn't do a lot of work. It's correct, right? But it doesn't do a lot of work for the assignment in terms of helping me to understand what she's doing. So to me, like, yeah, Yaroslav, if you found me on a nice day, I'd agree with you. It's a three. And if you found me on a mean day, I might give it a two. But it's somewhere on that range. It's somewhere on that range for me. it's missing a lot of stuff that would really help. It was a little bit more detailed. It really lift up the rest of the assignment a lot more. And so it's kind of like it's pretty load-bearing, and the assignment really, really needs that load-bearing stuff. And then it's not really super strong load-bearing stuff. And so that might be where it kind of gets pushed over into it too. It's like, oh, you know, you're really leaning on this code, and it's just okay. Yeah, undrawl. So what if for the code, if the student had clearly explained how they're using the packages and then you said that it requires you to refer to the documents and stuff. That would really help. That would really help me lock in this three. What I'm saying is I might be nice and go to the three, but if you did exactly what you said, where it's just like, hey, I used all these packages. Here are what they do. Here are where they're documented. Here's the mathematics going on inside them. then the code is much less load-bearing, and so it's much easier for me to give it a 3, because it's like, yeah, it does, okay, great. This is the, it's doing the thing you need of it now. Does that make sense in real? It doesn't, she doesn't have to write everything from scratch, she doesn't have to do everything like that to get that through. It's just like, a little more documentation here would really help secure this. Okay, how about explanation? I have a quick question. Sure, sure, sure. Yeah, what would you look for, like, for the code? For code? For fours, I usually look for stuff written from scratch. Or it depends if this is something that is more common in second or third assignments, when you've had more time to build some stuff. So for one, it's usually written from scratch. but in the second or third assignments, especially, you know, just sometimes the sheer weight of your pipeline, if you're computing 10 different models and you're doing all these different comparisons and you have all of these visualizations, visualizations sometimes that people use, have like, like I've seen people build like interactable visualizations and things like that. Then it's like, oh, well, you know, they did a lot of extra work here, right? The code is doing a lot of heavy lifting for the explanation. The code is doing a lot of cool comparisons. I get a lot more results out of this. And so then I would tend to bump it up to a four, essentially. So usually it's someone went deep and wrote something from scratch. Sometimes it's that someone just like-- they're mostly using packages, but they explain them all really well. And boy, howdy, there are a lot of them. All right, anyone want to guess explanation now? yeah jonathan i'd say a two to three in that there are some nice explanations for example saying that a cnn would do a better job than this particular model because it has this is this but then like in that specific example i would have liked to see a reference to like the bias you know or the variance um like she she mentions having such a small data set, and how does that actually affect the variance and so some more specificity and ML-relevant vocab? Yep. This, for me, I'm more or less right there with you, Jonathan, somewhere between a two or a three. I would probably lean toward two here. The thing that really breaks my heart here is I really love this assignment. I really think this is a cool comparison. I like that this was her choice. I like that this is the stuff she does. I want to give better scores for this. The question is, did she explain the models that she used? And the answer is, some of them, not really. How does this work? Right? It's the backbone of her piece. How does it work? Not really explained. PNN, how does that work? Not really explained. Right? The basic comparison that she did, like, it's even easy sort of to miss. Like when I get, I got to this thing and it's like, okay, there's a little bit of explanation explaining that this is her and Paul and things like that. Why didn't she explain why there are nine of these, right? Like this is, this is really, this got really confusing, right? So there are pieces of this that I really, really like. And there are pieces of it that are really, really pulling it down. And so again, I think I agree with Jonathan. It's not sort of on a two, three range. I think I lean a little more toward the two on explanation and maybe a little more toward the three on code, But they're both kind of on that like 2.5 territory sort of thing. Yeah. Yeah. Dayim. Yeah. I want to ask how strict is the word limit? Because I feel like if we actually want to get for that, it'll be way beyond one fifth. The way I feel, the way I feel about the word limit is it's okay to go over the word limit. What I'm saying with the word limit is I have seen very good assignments that don't use more words than this. Right. If you go over the word, I also think it's also like this word limit is sort of like, this is sort of what I like, I expect you can totally get threes around this word limit. If you go a little bit over that word limit, that might help. They're diminishing returns toward the high end of that word limit. Basically like after that, you'll, it'll, you know, you might need a little bit extra to kind of get all the things that you want to document. And then after that, going to start to trail off. I'll show you a very very long example of one of these from in for assignment two or three where the student just ran like like 25 different models against it and that is huge. I think the paper is like 80 pages long or something ridiculous like that but I like that one pretty well just because she did so much. She did so many different comparisons right. A lot of what's going on there though is like the comparisons are mostly kind of doing the same thing. At the very beginning of her paper she has a lovely summary with all of her findings that shows like, okay, these are the percentages of performance across these 10 different, 20 different models, right? And so all I needed to do is read those first few pages to kind of understood what she did, and then everything else is almost an appendix, right? So that's usually like, I've seen some really big ones, and I won't grade you down for really big ones, but it's also certainly not necessary that it go on and on and on. Yeah, Jaroslav. I was wondering if it would also help we talked about the quality of our data, like our sample size, like some explorer data analysis, basically like we talked in the class, don't get caught. Yeah, I love that stuff. I love that stuff. It really helps your explanation. It really just puts it on firmer footing. And depending, there could even be some flexibility that you add in there based on what kind of analyses you're doing on your data, right? It's an option, an opportunity to do a few of these kind of, you know, simpler statistical analysis t-tests and things like that to prove that, you know, all these two data samples look the same, that sort of stuff um that tends to help you know tends to shore up your your paper a little bit yeah dylan um this is very helpful and thank you for this but i was also wondering if there's any other student that submitted that got a higher course just so we could have like a comparison as well well i'm glad that you you asked this i've uploaded it Where the hell is this thing? Here it is. Okay. This is an example from another student in the same year, Bira. She did a really fun example where she did some sentiment analysis on her poetry journal. So she had a bunch of poetry that she had done. And so she loaded it, you know, loads it in. I'll go much quicker on this one. And she does some preprocessing where she explains how she vectorizes them with TF-IDF. Here she implements that TFIDF. She applies the vectorizer. She describes the algorithm. Here she says I'm going to use k-means clustering. Here's how you do k-means clustering. She writes a little equation here and explains what all the variables are. Okay, now I'm going to compute this silhouette score on my data. Here's how I did that. Here's her code for it. And now she's going to say, I'm going to run hyperparameter tuning. I'm going to try different cluster sizes and see where I get the best performance. She runs it a bunch of different clusters. She sees it kind of falls down here. She grabs like 10 clusters. She says, okay, here are my clusters for my poems. I'm going to put my poems into groups based on the words that they use in common. And so now here's my list of, okay, so here's the first cluster. It tends to use words like this. Here's the second cluster of my poetry. It tends to use words like this. Pretty neat, right? And then she says, okay, now I'm going to build a classifier. Can I classify poetry into what cluster it's in on the basis of the words that it uses? She shows she can get good performance on that. She has PCA. This is something we cover later in the class. We haven't covered it yet. Dimensionality reduction technique. Can I help to use it to visualize my data? Okay, so I'm going to project this data onto these two different things. Okay, so I try to cluster my poems. They're kind of all over the place. These elemental ones are kind of off by themselves. That's interesting, right? Oh, look, here's a very nice pipeline that shows all of the things that I did. And I would hear some functions and her reference. So a little bit of a longer paper, a little bit, it's closer to, this is a little bit, it's pretty close to the word count, actually. though because a lot of it is just code and visualizations. You can see how she used a lot of the pipeline itself to do those comparisons. Yeah, Grant, last question. Just in general, how much of the code do you want actually in the explanation like this? Generally, I find whenever I'm writing, it gets confusing to have all the code while I'm writing and generally like to do an appendix and then reference it, but I don't know if you have a preference against that. The appendix system is really useful. You could also do things like set flags to make it less verbose if it's throwing up a bunch of Linux into your pipeline. You can do less verbose things. It's useful to have the code in line, and I don't mind skipping over the code and having it mixed in. The thing for appendices is if you're calling certain functions or you want to write out things from scratch, that's a good thing to put in the appendix. The other thing, sometimes students have done this. Again, this is more common in assignments 2 and 3 when people's data sets are getting bigger. They'll write something, put it in appendix, run the codebook in the appendix. Put that run codebook on GitHub. And then put just the function of the code in the PDF that they turn in. and link me to GitHub where it's like, okay, it's like, I trained my model on this thing. If you want to see the full training stack, you can go look at it on GitHub, right? If the training stack has become like 100 pages of processing or something like that, you can go see that on GitHub or in my CoLab notebook, but I'm just turning in, here's the actual code and here's the results of it, right? That's totally fine. That's a totally fine way to do it. That's usually how I see appendices used. And I like that just fine. You're pretty free and flexible in the way you do it. I gave you a nice possible outline for your CoLab notebook. If you stick to that closely, that's fine. If you want to add some flourishes, I'm happy with that. All right. All right. So, because we're at time, let's wrap up with a really quick one of these. So, you've graded one yourself. What are your plans for your first assignment? How has this changed the way that you look at it? Give me a quick synthesis of some of your ideas now that you know what it's like to be in the evaluator's chair. I'll immediately put more LaTeX. It never hurts, Carl. It never hurts. Thank you. Thank you. In the interest of time, skip my usual wrap up. You all back here next week. My major recommendation over the break is take a couple of sessions to work on this assignment and really focus on just making sure that you got all the basics in there. Once you've got all the basics in there, you have two other assignments on which to build on top of this and to make it a truly incredible thing. And that's usually what I see over the course of the class. On this first one, just make sure you're starting with a rock solid foundation. Don't worry too much about making it spectacular. I'm sure some of you will do some spectacular stuff on this first assignment, and I'm excited to see it. Also, one of my favorite students I remember is someone who was a little bit nervous about this whole class. And then on the first assignment, got all threes and was ecstatic about it because that student was so worried that they were going to get a two. And they didn't. They built this rock solid assignment. I couldn't find anything wrong with it. And so I highlighted that assignment as like, this is what I would like people to start thinking about doing. Like, absolutely no misses here. So don't worry about winning everything. You got plenty of time for that. You got many races to run. Just make sure this one is a nice, solid one that you're proud of. All right. Have a wonderful break. I'll see you back here on next Wednesday. If you have any further questions, you can message me on Slack. I'll see you then. Thank you, Professor. Thank you. Thank you. Let's see. Vilna, how about you first? Yeah, I just have a quick question. I was just wondering if you recommend or not recommend to go with this multi-label approach. because what I'm doing is I have basically photos of foods that I was taking and I was trying to classify them like protein, meeting the bingo and then not meeting and then like carbs, etc. So in some cases it can be both, but should I maybe like try to eliminate the data that has both? There's a couple of ways to approach this. The one that I recommend the most for this first assignment especially is make different binary classifiers for each of your goals. So say like, okay, I'm going to do, I like the way you set this up. And you start out with like the protein, no protein classifier. And then you build the carb, no carb classifier. And then you build the fat, no fat classifier, right? And then when you print out your data, you see how accurate it is at each of those different things. Because as you say, food's going to have different properties. but you just build one really simple classifier for each of those properties. You could also then compare that data. I recommend doing that with logistic regression or something, nice simple binary classifiers first. Trees, right, stuff like that. Then you can compare that data to multi-label classifiers. So this is a good thing for reaching out for doing flexibility, right? If you get a multi-layer classifier, XGBoost is an example of this, but also something like just a tree, a multi-label tree, like a three leaf tree. So you do that. And then you say like, oh, look, when I was using the multi-label classifier, it was less accurate because it would get confused about these foods that belonged in multiple categories. It didn't really know how to do that as well. Whereas the binary classifier was maybe a little clearer. Or maybe the result comes out the other way. Either way, I'm interested. I want to know. That's so interesting, right? And that's the sign of a good assignment. Like you have that comparison between those two different ways of doing things. And you don't really care how the data comes out. As long as the, either way it comes out, you learn something about what the information contained in your pictures is. So, sounds great. Yeah, that makes sense. Thank you so much. Also, just a quick question out there. What if the photo will have, like, both? Like, I have two meals just right next to each other. It will probably get confused, right? Yeah, well, these are good cases where what you can do is you can then say, like, here's a nice thing to do. you go and you look at all the cases that are misclassified and you say, these are all the ones that I got wrong. And you visualize them. I say, here are all the ones that got right. Or here's the sample of the ones that got right. Right. Here's the sample of the ones that got wrong. Sure enough, it's misclassifying all the two meals ones, right? It doesn't know what to do with that. And then assignment two, you can like, there's other things you can do. You can do object recognition to like cut out individual chunks from the images and make a new data set. There's some stuff you can do to standardize the data. You can start using features like pre-trained CNNs and things like that to really get additional features. Great pipeline for assignment two, great pipeline for assignment three. This is a cool trajectory that you're going along. You don't have to do everything right now. For now, just visualizing, here's the bad ones. It's great. Thank you so much. Have a good day. Hello. Hello. I just have a question again about the... the size of my data because I, for the first one, I'm doing kind of like the BPM finder that I mentioned with you using the regression line. And what I have is that I have samples with multiple sorts of BPMs, both with tap-dance improvisation and also tap-dance improvisation with the metronome and without the metronome and also samples that are just like me doing steps on the actual beat and different like time signatures. And so far I have like 120 samples and I already did the processing. So I already transformed them into like spectrograms and did the whole like file transformation and all of that. I haven't done the feature engineering. But I'm just concerned about like the future and like the other assignments and then other possibilities for me to get like just samples from videos that I have in choreography videos to which I know the actual BPM of the song. But then the problem is that there's going to be noise on music and tap dance. So I was rethinking the whole pipeline and thinking if I should do something more simple such as does this audio has tap dance or not. But I really want to do the BPM thing just because I tested on existing BPM finders, and they suck with Tethys. They really suck. "