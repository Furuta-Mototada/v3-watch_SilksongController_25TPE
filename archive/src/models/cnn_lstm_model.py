"""
CNN/LSTM Hybrid Model for Gesture Recognition - Phase V

This module implements a state-of-the-art deep learning architecture combining:
- Convolutional layers for spatial feature extraction
- LSTM layers for temporal sequence modeling
- Dense layers for final classification

The model operates on raw sensor data (accelerometer, gyroscope, rotation)
without requiring manual feature engineering.

Architecture:
    Input (50 timesteps × 9 features)
    → Conv1D (feature extraction)
    → LSTM (temporal modeling)
    → Dense (classification)
    → Output (5 gesture probabilities)
"""

import numpy as np
import pandas as pd
from typing import Tuple, List, Optional

# TensorFlow/Keras imports (will be available after requirements update)
try:
    from tensorflow import keras
    from tensorflow.keras import layers, models
    from tensorflow.keras.utils import to_categorical
    KERAS_AVAILABLE = True
except ImportError:
    KERAS_AVAILABLE = False
    print("Warning: TensorFlow/Keras not available. Install with: pip install tensorflow")


# ==================== MODEL ARCHITECTURE ====================

def create_cnn_lstm_model(
    input_shape: Tuple[int, int] = (50, 9),
    num_classes: int = 5,
    cnn_filters: Tuple[int, int] = (64, 128),
    lstm_units: Tuple[int, int] = (128, 64),
    dense_units: int = 64,
    dropout_rate: float = 0.3
) -> 'keras.Model':
    """
    Creates a CNN/LSTM hybrid model for gesture recognition.
    
    This architecture is specifically designed for time-series sensor data:
    - CNN layers extract local patterns (feature learning)
    - LSTM layers model temporal dependencies (sequence memory)
    - Dense layers perform final classification
    
    Args:
        input_shape: (timesteps, features), default (50, 9)
                    50 timesteps = 1 second at 50Hz
                    9 features = 3 sensors × 3 axes
        num_classes: Number of gesture classes (default: 5)
                    [jump, punch, turn, walk, noise]
        cnn_filters: Number of filters in Conv1D layers (default: (64, 128))
        lstm_units: Number of units in LSTM layers (default: (128, 64))
        dense_units: Number of units in Dense layer (default: 64)
        dropout_rate: Dropout rate for regularization (default: 0.3)
    
    Returns:
        Compiled Keras model ready for training
    
    Model Summary:
        Layer 1-3: Conv1D → BatchNorm → MaxPooling (feature extraction)
        Layer 4-6: Conv1D → BatchNorm → MaxPooling (deeper features)
        Layer 7-8: LSTM → Dropout (temporal modeling)
        Layer 9-10: LSTM → Dropout (sequence aggregation)
        Layer 11-12: Dense → Dropout (classification prep)
        Layer 13: Dense (softmax output)
    
    Example:
        >>> model = create_cnn_lstm_model()
        >>> model.summary()
        >>> # Train model
        >>> model.fit(X_train, y_train, epochs=50, validation_split=0.2)
    """
    if not KERAS_AVAILABLE:
        raise ImportError("TensorFlow/Keras is required. Install with: pip install tensorflow")
    
    model = models.Sequential([
        # Input layer (implicit from first Conv1D)
        
        # ===== CNN Feature Extraction =====
        # First convolutional block
        layers.Conv1D(
            filters=cnn_filters[0],
            kernel_size=5,
            activation='relu',
            input_shape=input_shape,
            name='conv1d_1'
        ),
        layers.BatchNormalization(name='batch_norm_1'),
        layers.MaxPooling1D(pool_size=2, name='max_pool_1'),
        
        # Second convolutional block
        layers.Conv1D(
            filters=cnn_filters[1],
            kernel_size=3,
            activation='relu',
            name='conv1d_2'
        ),
        layers.BatchNormalization(name='batch_norm_2'),
        layers.MaxPooling1D(pool_size=2, name='max_pool_2'),
        
        # ===== LSTM Temporal Processing =====
        # First LSTM layer (return sequences for second LSTM)
        layers.LSTM(
            units=lstm_units[0],
            return_sequences=True,
            name='lstm_1'
        ),
        layers.Dropout(dropout_rate, name='dropout_1'),
        
        # Second LSTM layer (return final state only)
        layers.LSTM(
            units=lstm_units[1],
            return_sequences=False,
            name='lstm_2'
        ),
        layers.Dropout(dropout_rate, name='dropout_2'),
        
        # ===== Dense Classification =====
        # Pre-classification dense layer
        layers.Dense(
            units=dense_units,
            activation='relu',
            name='dense_1'
        ),
        layers.Dropout(dropout_rate, name='dropout_3'),
        
        # Output layer
        layers.Dense(
            units=num_classes,
            activation='softmax',
            name='output'
        )
    ], name='CNN_LSTM_Gesture_Model')
    
    # Compile model
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


# ==================== DATA PREPARATION ====================

def prepare_data_for_training(
    sensor_data: pd.DataFrame,
    labels_data: pd.DataFrame,
    window_size: int = 50,
    stride: int = 25,
    sensor_columns: Optional[List[str]] = None
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Prepares continuous sensor data for CNN/LSTM training using sliding windows.
    
    This function converts continuous recordings with labels into windowed samples
    suitable for training. It handles:
    - Sensor data processing (pivoting separate sensor rows)
    - Sliding window extraction
    - Label assignment for each window
    - Feature ordering and normalization
    
    Args:
        sensor_data: DataFrame with columns [timestamp, sensor, accel_x, accel_y, accel_z,
                    gyro_x, gyro_y, gyro_z, rot_x, rot_y, rot_z, rot_w]
                    NOTE: Sensor data has separate rows for each sensor type
        labels_data: DataFrame with columns [timestamp, gesture, duration]
        window_size: Number of timesteps per window (default: 50 = 1 second at 50Hz)
        stride: Step size for sliding window (default: 25 = 50% overlap)
        sensor_columns: List of sensor column names to use (default: all 10 sensors)
    
    Returns:
        X: np.ndarray of shape (num_samples, window_size, num_features)
        y: np.ndarray of shape (num_samples, num_classes) - one-hot encoded
    
    Example:
        >>> sensor_df = pd.read_csv('session_01.csv')
        >>> labels_df = pd.read_csv('session_01_labels.csv')
        >>> X, y = prepare_data_for_training(sensor_df, labels_df)
        >>> print(f"Generated {len(X)} training samples")
    """
    if sensor_columns is None:
        # Default: use all 10 sensor axes (including rot_w)
        sensor_columns = [
            'accel_x', 'accel_y', 'accel_z',
            'gyro_x', 'gyro_y', 'gyro_z',
            'rot_w', 'rot_x', 'rot_y', 'rot_z'
        ]
    
    # Gesture to index mapping
    gesture_to_idx = {
        'jump': 0,
        'punch': 1,
        'turn': 2,
        'walk': 3,
        'noise': 4
    }
    
    # Process sensor data: separate rows per sensor need to be merged
    # Separate by sensor type
    if 'sensor' in sensor_data.columns:
        accel_data = sensor_data[sensor_data['sensor'] == 'linear_acceleration'][
            ['timestamp', 'accel_x', 'accel_y', 'accel_z']
        ].copy()
        gyro_data = sensor_data[sensor_data['sensor'] == 'gyroscope'][
            ['timestamp', 'gyro_x', 'gyro_y', 'gyro_z']
        ].copy()
        rot_data = sensor_data[sensor_data['sensor'] == 'rotation_vector'][
            ['timestamp', 'rot_w', 'rot_x', 'rot_y', 'rot_z']
        ].copy()
        
        # Get all unique timestamps
        all_timestamps = pd.DataFrame({'timestamp': sorted(sensor_data['timestamp'].unique())})
        
        # Merge all sensors on timestamp
        sensor_processed = all_timestamps.copy()
        sensor_processed = sensor_processed.merge(accel_data, on='timestamp', how='left')
        sensor_processed = sensor_processed.merge(gyro_data, on='timestamp', how='left')
        sensor_processed = sensor_processed.merge(rot_data, on='timestamp', how='left')
        
        # Forward-fill to propagate sensor values (sensors update at different rates)
        sensor_processed[sensor_columns] = sensor_processed[sensor_columns].ffill()
        
        # Fill any remaining NaN (at the beginning) with 0
        sensor_processed[sensor_columns] = sensor_processed[sensor_columns].fillna(0)
    else:
        # Data is already processed
        sensor_processed = sensor_data.copy()
    
    # Sort data by timestamp
    sensor_processed = sensor_processed.sort_values('timestamp').reset_index(drop=True)
    labels_data = labels_data.sort_values('timestamp').reset_index(drop=True)
    
    # Extract windows
    X_windows = []
    y_labels = []
    
    # Create sliding windows
    for start_idx in range(0, len(sensor_processed) - window_size, stride):
        end_idx = start_idx + window_size
        
        # Extract window
        window = sensor_processed.iloc[start_idx:end_idx]
        
        # Get timestamp at center of window
        center_time = window['timestamp'].iloc[window_size // 2]
        
        # Find corresponding label
        label = None
        for _, label_row in labels_data.iterrows():
            label_start = label_row['timestamp']
            label_end = label_start + label_row['duration']
            
            if label_start <= center_time < label_end:
                label = label_row['gesture']
                break
        
        if label is None:
            continue  # Skip windows without labels
        
        # Extract features
        try:
            features = window[sensor_columns].values
            
            # Check if window has correct shape
            if features.shape[0] != window_size:
                continue
            
            X_windows.append(features)
            y_labels.append(gesture_to_idx[label])
            
        except KeyError:
            # Some sensor columns might be missing, skip this window
            continue
    
    if len(X_windows) == 0:
        raise ValueError("No valid windows could be extracted. Check sensor data format.")
    
    # Convert to numpy arrays
    X = np.array(X_windows)
    y = to_categorical(y_labels, num_classes=5)
    
    return X, y


def create_sliding_window_dataset(
    sensor_file: str,
    labels_file: str,
    window_size: int = 50,
    stride: int = 25
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Convenience function to load data from files and prepare for training.
    
    Args:
        sensor_file: Path to sensor data CSV
        labels_file: Path to labels CSV
        window_size: Window size in timesteps
        stride: Stride for sliding window
    
    Returns:
        X, y: Training data ready for model.fit()
    
    Example:
        >>> X, y = create_sliding_window_dataset(
        ...     'data/continuous/session_01.csv',
        ...     'data/continuous/session_01_labels.csv'
        ... )
        >>> model.fit(X, y, epochs=50, validation_split=0.2)
    """
    sensor_data = pd.read_csv(sensor_file)
    labels_data = pd.read_csv(labels_file)
    
    return prepare_data_for_training(
        sensor_data,
        labels_data,
        window_size=window_size,
        stride=stride
    )


# ==================== MODEL UTILITIES ====================

def save_model(model: 'keras.Model', filepath: str):
    """
    Save trained model to file.
    
    Args:
        model: Trained Keras model
        filepath: Path to save model (e.g., 'models/cnn_lstm_gesture.h5')
    """
    if not KERAS_AVAILABLE:
        raise ImportError("TensorFlow/Keras is required")
    
    model.save(filepath)
    print(f"Model saved to {filepath}")


def load_model(filepath: str) -> 'keras.Model':
    """
    Load trained model from file.
    
    Args:
        filepath: Path to saved model
    
    Returns:
        Loaded Keras model
    """
    if not KERAS_AVAILABLE:
        raise ImportError("TensorFlow/Keras is required")
    
    model = keras.models.load_model(filepath)
    print(f"Model loaded from {filepath}")
    return model


def predict_gesture(
    model: 'keras.Model',
    sensor_window: np.ndarray,
    gesture_names: Optional[List[str]] = None
) -> Tuple[str, float]:
    """
    Predict gesture from sensor window.
    
    Args:
        model: Trained CNN/LSTM model
        sensor_window: Sensor data of shape (window_size, num_features)
        gesture_names: List of gesture names (default: standard 5 gestures)
    
    Returns:
        (gesture_name, confidence): Predicted gesture and confidence score
    
    Example:
        >>> prediction, confidence = predict_gesture(model, sensor_window)
        >>> print(f"Predicted: {prediction} ({confidence:.2%})")
    """
    if gesture_names is None:
        gesture_names = ['jump', 'punch', 'turn', 'walk', 'noise']
    
    # Ensure correct input shape
    if len(sensor_window.shape) == 2:
        sensor_window = np.expand_dims(sensor_window, axis=0)
    
    # Predict
    prediction = model.predict(sensor_window, verbose=0)
    
    # Get result
    gesture_idx = np.argmax(prediction[0])
    confidence = prediction[0][gesture_idx]
    
    return gesture_names[gesture_idx], confidence


# ==================== MODEL INFO ====================

def print_model_info():
    """Print information about the CNN/LSTM architecture"""
    info = """
╔══════════════════════════════════════════════════════════════╗
║        CNN/LSTM Gesture Recognition Model - Phase V          ║
╚══════════════════════════════════════════════════════════════╝

Architecture Overview:
  Input: (50 timesteps, 9 features)
    - 50 timesteps = 1 second at 50Hz sampling rate
    - 9 features = accelerometer (3) + gyroscope (3) + rotation (3)
  
  Stage 1: Convolutional Feature Extraction
    - Conv1D(64 filters, kernel=5) + BatchNorm + MaxPool
    - Conv1D(128 filters, kernel=3) + BatchNorm + MaxPool
    → Learns spatial patterns in sensor data
  
  Stage 2: LSTM Temporal Modeling
    - LSTM(128 units, return_sequences=True) + Dropout(0.3)
    - LSTM(64 units) + Dropout(0.3)
    → Learns temporal dependencies and transitions
  
  Stage 3: Dense Classification
    - Dense(64 units, ReLU) + Dropout(0.3)
    - Dense(5 units, Softmax)
    → Final gesture classification

Output: Probabilities for 5 gestures
  [jump, punch, turn, walk, noise]

Model Size: ~214K parameters (~850 KB)
Inference Speed: 10-30ms per prediction (CPU)
Expected Accuracy: 90-98% on test set

Usage:
  from models.cnn_lstm_model import create_cnn_lstm_model
  
  model = create_cnn_lstm_model()
  model.fit(X_train, y_train, epochs=50, validation_split=0.2)
  model.save('models/cnn_lstm_gesture.h5')
"""
    print(info)


if __name__ == "__main__":
    # Print model info when run directly
    print_model_info()
    
    if KERAS_AVAILABLE:
        # Create and display model
        print("\nCreating model...")
        model = create_cnn_lstm_model()
        print("\nModel Summary:")
        model.summary()
    else:
        print("\n⚠️  TensorFlow not installed. Install with:")
        print("    pip install tensorflow>=2.10.0")
